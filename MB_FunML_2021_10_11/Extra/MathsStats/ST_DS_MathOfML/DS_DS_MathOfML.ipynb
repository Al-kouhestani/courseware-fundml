{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mathematics of Machine Learning\n",
    "## Data Science for Data Scientists\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mathematics & Method of Supervised Learning\n",
    "\n",
    "This module helps lay the *conceptual* foundations of supervised machine learning. It is unlikely you will ever write code which looks exactly like this, you may, but more commonly you will rely on libraries which use these ideas and techniques.\n",
    "\n",
    "It is important to understand the general appraoch to be able to use these libraries competently, and to understand the results they provide. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For example LinearRegression is: empirical loss minimization with a square loss. Lasso regression is empirical loss minimization with a square regularization term. \n",
    "\n",
    "These terms should hopefully get clearer in the examples below, the key idea here is that \"all this work is done for you\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Set Up of Supervised Machine Learning in terms of Loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "a true relationship provides some true data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x_age = [18, 19, 20, 21]\n",
    "\n",
    "# true relationship between age and height\n",
    "def f(x):\n",
    "    return 0.1*x + 0.01\n",
    "\n",
    "y_true_height = [f(x) for x in x_age]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "an estimate for the true relationship provides some estimate points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def fhat(x, a=0.1, b=0.05):\n",
    "    return a*x + b\n",
    "\n",
    "\n",
    "yhat = [fhat(x) for x in x_age]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.85, 1.9500000000000002, 2.05, 2.15] [1.81, 1.9100000000000001, 2.01, 2.11]\n"
     ]
    }
   ],
   "source": [
    "print(yhat, y_true_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "we can define a loss *per point* which tells us how far away our estimates are from our predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def loss_point(prediction, observation):\n",
    "    return abs(prediction - observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "the total loss is the sum over all points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16000000000000014"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([loss_point(p, o) for p, o in zip(yhat, y_true_height)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "since we are comparing our estimates to the true values, all errors are due to the *model* being inexact (rather than some \"inherent\" errors in the dataset).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loss Minimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We rephrase the loss formula so it computes the total loss from the parameters direcly, ie., without having to precompute the predictions first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def loss_fn(f_est, f_true, a, b):\n",
    "    y = [f_true(x) for x in x_age]\n",
    "    yhat = [f_est(x, a, b) for x in x_age]\n",
    "    return sum([loss_point(p, o) for p, o in zip(yhat, y)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "With this formulation, you vary the parameters until the loss in minimal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(fhat, f, 0.1, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Suppose the true function were quadratic (ie., $ax^2 +bx + c$) then there would be a *necessary* true loss, due to using a poorly representative model. So \"true loss\" is a guide to how good the model is only, and does not arise from issues with the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Empirical Loss Minimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In practice, we do not have $f$. \n",
    "\n",
    "We have dataset $\\mathcal{D}$ which we *hope* \"resembles\" a true relationship $f$. The overall goal is to find an ideal, ie., *optimal*, estimate for f, $\\hat{f}$, **using this dataset**.\n",
    "\n",
    "How? Formulate a loss which describes how \"close\" $\\hat{f}$ is to $\\mathcal{D}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def loss_empirical(D, fhat, a, b):\n",
    "    Dx, Dy = D\n",
    "    yhat = [fhat(x, a, b) for x in Dx]\n",
    "    return sum([loss_point(p, o) for p, o in zip(yhat, Dy)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We assume $\\mathcal{D}$ is generated from a true model, just with some random error..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from random import random as rerror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Dx_school = (2, 3, 4)\n",
    "Dy_school = [ 2*x + 1 + rerror() for x in Dx_school ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Here the true model is $f(x) = 2x + 1$ .\n",
    "\n",
    "In practice we wouldnt know this, we would just have $\\mathcal{D}$ as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "D_school = [Dx_school, Dy_school]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 3, 4), [5.204371643320275, 7.614282900476174, 9.732090790868954]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_school"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Our model, $\\hat{g}$ will be linear, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def ghat(x, a, b):\n",
    "    return a*x + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Even if we use this model with the \"true\" parameters, the *empirical loss* still shows an error. Now due to the dataset cotaining \"random\" variation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.550745334665403"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_empirical(D_school, ghat, 2, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The danger here is that we optimize for the dataset and thereby arrive at parameters very different from the truth.\n",
    "\n",
    "This is unlikely to be serious in the cases where linear models, etc. direclty & obviously apply. Issue is more common in complex datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How do you choose the best parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Insight: the rate of change of the total loss is a guide to how fast to change the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "eg., suppose you move too far in the wrong direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6420020480248523"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_empirical(D_school, ghat, 2, 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.050745334665403"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_empirical(D_school, ghat, 2, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "You went too far, ie., lowered b too much. Perhaps stop at your best previous $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Pace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Big changes to the loss suggest you should make big changes to the parameters, ie., use some percetage of the total loss as *how much* you're going to change the total parameter.\n",
    "\n",
    "Below, we update $b$ using 10% of the total loss of the previous step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Start at some random initial values for $a, b$..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.4492546653346"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_empirical(D_school, ghat, 2, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Choose a parameter to optimize, say $b$, and move by 10%..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.6492546653346"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_empirical(D_school, ghat, 2, 10 - 2.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.859254665334599"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_empirical(D_school, ghat, 2, 10 - 2.6 - 1.8 - 1.2 - 0.93) # 5 steps in one line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9000020480248514"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_empirical(D_school, ghat, 2, 10 - 2.6 - 1.8 - 1.2 - 0.93 - 0.65 - 0.45 - 0.32 - 0.26 - 0.148 - 0.1 - 0.08 - 0.08 - 0.07 - 0.07)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "At some point the loss changes direction, ie., increases again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0047453346654018"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_empirical(D_school, ghat, 2, 10 - 2.6 - 1.8 - 1.2 - 0.93 - 0.65 - 0.45 - 0.32 - 0.26 - 0.148 - 0.1 - 0.08 - 0.08 - 0.07 - 0.07 - 0.06)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here, we stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "b_optimal = 10 - 2.6 - 1.8 - 1.2 - 0.93 - 0.65 - 0.45 - 0.32 - 0.26 - 0.148 - 0.1 - 0.08 - 0.08 - 0.07 - 0.07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.242"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_optimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In mathematical notation optimial values are sometimes written with a star, here: $b^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "NB. We cheated by starting with the optimal $a$ and just optimizing $b$... to do this properly we need a more sophisticated algoithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise\n",
    "\n",
    "Convince yourself you understand the set up and by-hand process of loss minimization.\n",
    "\n",
    "* define a true function `h`, eg., 2x + 3\n",
    "* define a estimate called `hhat`, 2.1x + 2.9\n",
    "* define a dataset for `x`, eg., `list(range(0, 10))`\n",
    "* generate a dataset for `y` and for `yhat`\n",
    "    * either loop over `x` and `.append` a `h(x)` to a `y` list\n",
    "    * or use a comprehension as above\n",
    "    \n",
    "* define a *square loss* per-point which is `(pred - obs) ** 2`\n",
    "* compute the true total loss \n",
    "    * run your per-point loss over all `y` and `yhat`\n",
    "    * `sum()`\n",
    "    \n",
    "* by-hand, update your `hhat` function to have different `a, b` values\n",
    "    * recompute the total loss\n",
    "    * update your `hhat` until the total loss is minimal\n",
    "        * you should be able to get `0` total loss, because you are comparing with the true function\n",
    "        \n",
    "* Stretch Exercise:\n",
    "    * as-above, with a dataset\n",
    "    * hint: copy/paste my `loss_empirical` function & generate your own dataset as I have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Remarks about the Loss Function\n",
    "\n",
    "There is no \"best\" formula for the loss... it is just any formula which tracks \"how well\" $\\hat{f}$ estimates the domain of interest. \n",
    "\n",
    "Some considerations: some loss formulae are more useful for by-hand mathematics or certain algorithms. Here the *square-loss* is *smooth* so its rate of change is continuous, ie., it will always change without gaps. Meaning it is \"safe and easy\" to use this rate of change of this loss to update my parameters. \n",
    "\n",
    "Aside: in practice, the absolute loss tends to perform better ie., provide better parameter estiamtes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The loss optimization processs is *empirical* ie., relative to a dataset and therefore cannot be \"trusted\" to yield the true result. \n",
    "\n",
    "The question for regularization is: Can we change the empirical loss formula to help?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def loss_empirical_old(D, fhat, a, b):\n",
    "    Dx, Dy = D\n",
    "    yhat = [fhat(x, a, b) for x in Dx]\n",
    "    return sum([(p - o)**2 for p, o in zip(yhat, Dy)])\n",
    "\n",
    "def loss_empirical_reg(D, fhat, a, b):\n",
    "    Dx, Dy = D\n",
    "    yhat = [fhat(x, a, b) for x in Dx]\n",
    "    return sum([(p - o)**2 + 0.15*(a + b)**2 for p, o in zip(yhat, Dy)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The above modification produces a *lower* loss for the true value of $b$,..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.109953234507925"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_empirical_reg(D_school, ghat, 2, 1.242)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.005068176485981"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_empirical_reg(D_school, ghat, 2, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "ie., we would now prefer $b^* = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "(Still, it is unlikely to settle on `1`...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.993618643019439"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_empirical_reg(D_school, ghat, 2, 1.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How does this work?\n",
    "\n",
    "We added a term to the loss which had *nothing to do* with the fit to dataset; ie., a term involving $a, b$ only.\n",
    "\n",
    "$a, b$ are the heart of the model, if you optimize for *something* to do with only $a, b$ then you impose some principles on the model itself: eg., that it should be simple, etc.\n",
    "\n",
    "Suppose there is some dataset error, eg., `2.5` adding the total of $(a + b)^2$  forces the error to increase. By forcing the model to consider both equally, it drags the model away from extreme values of $a, b$ which tends to be caused by misfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For the same dataset error, higher values for both parameters is penalized.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.110000000000001"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2.5 + (2 + 1.1) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.740000000000002"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2.5 + (2 + 1.2) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.740000000000002"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2.5 + (2.1 + 1.1) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How much to penalize these high-values is a hyper-parameter, ie., up to you to decide. Below, 15% of the term is used.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.9415000000000004"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2.5 + 0.15*(2 + 1.1) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0360000000000005"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2.5 + 0.15*(2 + 1.2) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "NB. recall, given the dataset error is fixed, we will chose a *lower* $b$  because the loss is lower. Here then, we would prefer $1.1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In practice, by choosing $1.1$ we may increase the dataset error: so there is a tug-of-war between dataset fit and \"balancing parameters\". The *strength* of the regularization, ie., how much simplicity wins is given by the 15% above.  At 0% the dataset determines everything.\n",
    "\n",
    "If the dataset error increases at 1.1, and we use only 15% of the regularization term, we still may prefer the higher $b$..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0415"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2.6 + 0.15*(2 + 1.1) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.936"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2.4 + 0.15*(2 + 1.2) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Aside: interpreting (a + b)^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Recall, $(a + b)^2 = a^2 + b^2 + 2ab$\n",
    "\n",
    "So the loss formula is, $l(a, b; x, y) = error_{data} + error_{reg} = (ax + b - y)^2 +  a^2 + b^2 + 2ab$\n",
    "\n",
    "Think about optimizing this formula, ie., finding its lowest value by choosing $a, b$. \n",
    "\n",
    "The ideal value for each term is $0$ or as low as possible. \n",
    "\n",
    "Forces driving the choice of $a,b$...\n",
    "\n",
    "1. $(ax + b - y)^2$ is zero/low when $ax + b = y$, so $y$ drives $a, b$ up \"to meet it\".\n",
    "2. $a^2$ is low when $a$ is low. \n",
    "3. $b^2$ is low when $b$ is low.\n",
    "4. $2ab$ is low when *both* $a$ and $b$ is low!\n",
    "\n",
    "Force `1` prefers $a, b$ to meet $y$. Forces `2, 3` prefer the lowest $a,b$ possible; and force `4` is happy only when  both $a, b$ is low.\n",
    "\n",
    "So $error_{reg}$ causes lowering $a, b$  individually to be optimial, and lowering *both at the same time*. This corresponds to models which don't \"put all their eggs in one basket\", ie., happen to get a good fit because of a good slope. It requires models to fit the data, \"and then\" try to balance that fit by both learning a non-extreme (ie., low) slope and an intercept. \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
